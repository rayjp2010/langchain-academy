{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ee9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue, [\"summarize_conversation\", END])\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"I am doing well, thank you for asking!  As a large language model, I don't experience emotions or feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--9b6c6321-dd86-4727-bdc4-ab00194cf981-0', usage_metadata={'input_tokens': 5, 'output_tokens': 52, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"how are you doing?\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"Hi Lance!  It's nice to properly meet you.  How's your day going so far?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--4ffa41a4-6acb-45a0-a0b3-95a14fd5f931-0', usage_metadata={'input_tokens': 170, 'output_tokens': 24, 'total_tokens': 194, 'input_token_details': {'cache_read': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "    # if 'conversation' in chunk:\n",
    "    #     print(\"Conversation:\")\n",
    "    #     chunk['conversation'][\"messages\"].pretty_print()\n",
    "    # elif 'summarize_conversation' in chunk:\n",
    "    #     print(\"Summary:\")\n",
    "    #     print(chunk['summarize_conversation']['summary'])\n",
    "    # else:\n",
    "    #     print(chunk)\n",
    "    # print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"hi! I'm Lance again.\", additional_kwargs={}, response_metadata={}, id='da172c5f-d872-4f2d-938c-60053c4f2a10'), AIMessage(content=\"Hi Lance.  While I appreciate you letting me know your name,  is there a question or task you'd like assistance with?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0efa5f04-0967-4a83-8cd7-1eda75b46357-0', usage_metadata={'input_tokens': 124, 'output_tokens': 29, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"hi! I'm Lance again.\", additional_kwargs={}, response_metadata={}, id='a9e97a9b-a785-4a31-b1f0-7f7d4715ee9a')], 'summary': 'The conversation continues to consist primarily of Lance repeatedly introducing himself as \"Lance.\"  The AI has repeatedly acknowledged his introduction, but attempts to move the conversation beyond the simple greeting have been unsuccessful.  There remains no substantive conversation or request for assistance from Lance.'}\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "{'messages': [HumanMessage(content=\"hi! I'm Lance again.\", additional_kwargs={}, response_metadata={}, id='da172c5f-d872-4f2d-938c-60053c4f2a10'), AIMessage(content=\"Hi Lance.  While I appreciate you letting me know your name,  is there a question or task you'd like assistance with?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0efa5f04-0967-4a83-8cd7-1eda75b46357-0', usage_metadata={'input_tokens': 124, 'output_tokens': 29, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"hi! I'm Lance again.\", additional_kwargs={}, response_metadata={}, id='a9e97a9b-a785-4a31-b1f0-7f7d4715ee9a'), AIMessage(content='Hello again, Lance.  Is there anything I can help you with today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--1c5e6dc7-282f-462f-b378-b77ea86db096-0', usage_metadata={'input_tokens': 100, 'output_tokens': 17, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}})], 'summary': 'The conversation continues to consist primarily of Lance repeatedly introducing himself as \"Lance.\"  The AI has repeatedly acknowledged his introduction, but attempts to move the conversation beyond the simple greeting have been unsuccessful.  There remains no substantive conversation or request for assistance from Lance.'}\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance again.\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    print(\"+++\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 11, 'output_tokens': 0, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' San Francisco 49ers are a professional American football team based in San Francisco', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=', California.  They are members of the National Football Conference (NFC) West', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\" division in the National Football League (NFL).\\n\\nHere's a summary of key aspects of the team:\\n\\n* **History:** Founded in 19\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='46 as the original San Francisco 49ers (they were one of the original teams in the All-America Football Conference before joining the NFL in ', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\"1950), they boast a rich and storied history.  They've won five Super Bowls (XVI, XIX, XXIII, XXIV, and XXIX), and are considered one of the most successful franchises in NFL history.  \", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='Their success is largely tied to legendary coaches like Bill Walsh and recent success under Kyle Shanahan.\\n\\n* **Current Status:**  The 49ers are currently a consistently competitive team, known for their strong defense and a usually potent running', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\" game.  They've made several deep playoff runs in recent years, showcasing a strong roster and coaching staff.  Their recent success has been built on a foundation of strong drafting and player development.\\n\\n* **Key Players (this is subject to change rapidly):**  The 49ers' roster is constantly\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\" evolving, but typically features a mix of established stars and promising young talent.  Recent key players have included (but are not limited to)  quarterback Brock Purdy, running back Christian McCaffrey, and defensive stars like Nick Bosa.  It's important to check current rosters for the most up-to\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\"-date information.\\n\\n* **Coaching Staff:**  Kyle Shanahan is the current head coach, known for his offensive prowess.  The coaching staff plays a significant role in the team's success.\\n\\n* **Stadium:** Levi's Stadium, located in Santa Clara, California, is their home stadium.\\n\\n\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content='* **Rivalries:** The 49ers have long-standing rivalries with teams like the Dallas Cowboys (dating back to the 1990s), the Seattle Seahawks (a more recent, intense rivalry within the NFC West), and the Green Bay Packers (a rivalry stemming from historical matchups and playoff encounters', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\").\\n\\nIn short, the San Francisco 49ers are a consistently strong NFL franchise with a rich history, a dedicated fan base, and a current team aiming for Super Bowl contention.  To get the most up-to-date information on their roster, standings, and news, it's best to consult\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=' reputable sports news websites and the official 49ers website.\\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--d2e760fb-808e-49ba-91f4-af7ac8e2b762', usage_metadata={'input_tokens': -1, 'output_tokens': 527, 'total_tokens': 526, 'input_token_details': {'cache_read': 0}})}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shintaro Koizumi (小泉 信太郎, *Koizumi Shintarō*,  January 8, 1926 – June 18, 2012) was a prominent Japanese politician who served as a member of the House of Representatives for many years.  He's perhaps best known for being the **father of Junichiro Koizumi**, the former Prime Minister of Japan.\n",
      "\n",
      "While Shintaro Koizumi held several ministerial posts throughout his career, including Minister of Posts and Telecommunications and Minister of Foreign Affairs, he never achieved the same level of national prominence as his son.  He was a member of the Liberal Democratic Party (LDP) and was known for his conservative views.  His political career, though long and influential within the LDP, is often overshadowed by his son's more impactful and widely recognized premiership.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Who is Koizumi Shintaro.\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, flush=True, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- 🚀 API: http://127.0.0.1:2024\n",
    "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All hosted graph names:\n",
      "- dynamic_breakpoints\n",
      "- agent\n",
      "\n",
      "Total graphs found: 2\n"
     ]
    }
   ],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()\n",
    "\n",
    "# Print all graph names\n",
    "print(\"All hosted graph names:\")\n",
    "for assistant in assistants:\n",
    "    print(f\"- {assistant['graph_id']}\")\n",
    "    \n",
    "print(f\"\\nTotal graphs found: {len(assistants)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '1f05fb9d-061e-6ff0-bfd3-83bfbc723667', 'attempt': 1})\n",
      "StreamPart(event='updates', data={'assistant': {'messages': [{'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run--e42553ec-3456-4ea9-904e-ff29f86d3b86-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': 'b7cd0ba1-571c-4ed2-9232-2a17ce56f04f', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 75, 'output_tokens': 5, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}}}]}})\n",
      "StreamPart(event='updates', data={'tools': {'messages': [{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '2904d843-5deb-49bb-a10e-c7702e0c5ba9', 'tool_call_id': 'b7cd0ba1-571c-4ed2-9232-2a17ce56f04f', 'artifact': None, 'status': 'success'}]}})\n",
      "StreamPart(event='updates', data={'assistant': {'messages': [{'content': 'The result is 6.', 'additional_kwargs': {}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run--9031067d-8105-47ff-bc5a-08427fb0d6d9-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 83, 'output_tokens': 7, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}}}]}})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"updates\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='fbc93458-b538-4604-b5ea-da76ad753912'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 75, 'output_tokens': 5, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}}, 'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--7be95f9e-9c90-497c-93eb-b48eea205527-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': 'd507374f-4351-429c-9e75-727385a4031e', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' name='multiply' id='c4b7ab9e-f39a-464b-a66e-7c297aa0f61b' tool_call_id='d507374f-4351-429c-9e75-727385a4031e'\n",
      "=========================\n",
      "content='The result is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 83, 'output_tokens': 7, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--43ea14d8-5dca-4152-aaff-e24baa00fe2e-0'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474e407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run--63d5c462-b25c-488a-b19f-a0999b8b6545', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '36f0ea67-db26-4950-9e64-6b064e41968b', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 75, 'output_tokens': 5, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}}}]\n",
      "[{'content': 'The', 'additional_kwargs': {}, 'response_metadata': {'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run--d3b96b27-1580-4a7b-b44e-ad3eafdd7bca', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 200, 'output_tokens': 0, 'total_tokens': 200, 'input_token_details': {'cache_read': 0}}}]\n",
      "[{'content': 'The result is 6.\\n', 'additional_kwargs': {}, 'response_metadata': {'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash'}, 'type': 'ai', 'name': None, 'id': 'run--d3b96b27-1580-4a7b-b44e-ad3eafdd7bca', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 83, 'output_tokens': 7, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}}}]\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    if event.event == 'messages/partial':\n",
    "        print(event.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 1f05fbac-57ab-683c-b741-122b533b1a89\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: 1d16905f-77a2-40a8-aafe-8bef06cec5f2, Function: multiply, Arguments: {'a': 2.0, 'b': 3.0}\n",
      "Response Metadata: Finish Reason - STOP\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The result is 6.\n",
      "\n",
      "Response Metadata: Finish Reason - STOP\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
